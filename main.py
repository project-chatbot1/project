# -*- coding: utf-8 -*-
"""Knowledge base chatbot using OpenAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DekshC48lz71pudHW3kOWSmNMCAdXe4w
"""

"""
!pip install langchain

!pip install gpt_index

"""

from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTVectorStoreIndex, LLMPredictor, PromptHelper
from llama_index import StorageContext, load_index_from_storage

from langchain import OpenAI
import sys
import os
import openai
from dotenv import load_dotenv
from typing import Union
import uvicorn
from flask_cors import CORS

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware


app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


load_dotenv()

openai.api_key = os.environ.get("OPENAI_API_KEY")

"""
from IPython.display import Markdown, display

"""
messages =[{"role": "system", "content": "You are an AI specialized in answering KNUST questions. Do not answer anything other than KNUST queries."}]


def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 300
    max_chunk_overlap = 0.5
    chunk_size_limit = 600

    # llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name="text-davinci-003", max_tokens=num_outputs))
    # prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    documents = SimpleDirectoryReader(r"C:\KnowledgeBase").load_data()

    index = GPTVectorStoreIndex.from_documents(documents)
    # storage_context = StorageContext.from_defaults()

    index.storage_context.persist(persist_dir=r"C:\Users\vamoa\OneDrive\Desktop\json\index.json")

    return index

@app.get("/")
def read_root():
    return {"Hello": "Worldie"}

@app.get("/query/{question}")
def ask_ai(question: str, q: Union[str, None] = None):
    # index = SimpleDirectoryReader("index.json").load_data()
    storage_context = StorageContext.from_defaults(persist_dir=r'C:\Users\vamoa\OneDrive\Desktop\json\index.json')
    index = load_index_from_storage(storage_context)

    query_engin = index.as_query_engine()

    # index = GPTVectorStoreIndex.load_from_disk('index.json')

    # while True:
    #     query = input("Hey, there KNUST Fresher, how can I serve you today? ")
    #     response = query_engin.query("Based on the information  in my custom knowledge base" + " " + query)
    #     # response = index.query(query, response_mode="compact")
    #     print(f"Response: {response.response}")

    # query = input("Hey, there KNUST Fresher, how can I serve you today? ")
   
    query = question #"When was KNUST built?"
    response = query_engin.query("Based on the information  in my custom knowledge base" + " " + query)
    # response = index.query(query, response_mode="compact")
    
    return {"Answer": response.response}
    
 

construct_index(r"C:\KnowledgeBase")

